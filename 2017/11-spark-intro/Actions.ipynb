{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce(func)\n",
    "\n",
    "Agrega os elementos do RDD usando uma função `func` (que leva dois argumentos e retorna um). A função deve ser [comutativa](https://pt.wikipedia.org/wiki/Comutatividade) e [associativa](https://pt.wikipedia.org/wiki/Associatividade) para que possa ser computada corretamente em paralelo.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 11))\n",
    "\n",
    "def summation(a, b): return a + b\n",
    "def max(a, b): return a if a > b else b\n",
    "\n",
    "# reduce to find the sum\n",
    "print (data.reduce(summation))\n",
    "# reduce to find the max\n",
    "print (data.reduce(max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## collect()\n",
    "\n",
    "Retornar todos os elementos do RDD como uma lista (do python, no caso). Isso geralmente é útil após um filtro ou outra operação que retorna um subconjunto suficientemente pequeno dos dados.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710, 720, 730, 740, 750, 760, 770, 780, 790, 800, 810, 820, 830, 840, 850, 860, 870, 880, 890, 900, 910, 920, 930, 940, 950, 960, 970, 980, 990, 1000]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 1001))\n",
    "print('data: ', data.filter(lambda i: i % 10 == 0).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count()\n",
    "\n",
    "Retorna o número de elementos no RDD.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  1000\n",
      "data filtered:  100\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 1001))\n",
    "print ('data: ', data.count())\n",
    "print ('data filtered: ', data.filter(lambda i: i % 10 == 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take(n)\n",
    "\n",
    "Retorna uma lista com os primeiros n elementos do RDD.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data first 10:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 1001))\n",
    "print ('data first 10: ', data.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## takeSample(withReplacement, num, [seed])\n",
    "\n",
    "Retorna uma lista com uma amostra aleatória de `num` elementos do RDD, com ou sem substituição (`withReplacement`), opcionalmente pré-especificando uma semente (`seed`) de gerador de números aleatórios.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 3, 6]\n",
      "[8, 6, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 11))\n",
    "# sampling with replacement\n",
    "print (data.takeSample(True, 4))\n",
    "# sampling without replacement\n",
    "print (data.takeSample(False, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saveAsTextFile(path)\t\n",
    "\n",
    "Escreve os elementos do RDD como um arquivo de texto (ou conjunto de arquivos de texto) em um determinado diretório no sistema de arquivos local, no HDFS ou qualquer outro sistema de arquivos suportado pelo Hadoop. O Spark chamará `toString` em cada elemento para convertê-lo em uma linha de texto no arquivo.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling o347.saveAsTextFile.\n",
       ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/home/abevieiramota/Documents/github-repos/data-science-cookbook/2017/11-spark-intro/range_1_to_100 already exists\n",
       "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1119)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1096)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1096)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1070)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n",
       "\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:550)\n",
       "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o347.saveAsTextFile.\\n', JavaObject id=o348), <traceback object at 0x7f2d06d415c8>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 1001))\n",
    "data.saveAsTextFile(\"range_1_to_100\")\n",
    "\n",
    "# Visualize o diretório deste notebook. Existirá um pasta com o nome range_1_to_100. \n",
    "# Nela haverá arquivos que do RDD. Cada partição do processamento tem um arquivo correspondente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## foreach(func)\n",
    "\n",
    "Executa uma função `func` para cada elemento do RDD. Isso geralmente é feito para operações sem tanto efeito, como atualizar um acumulador ou interagir com sistemas de armazenamento externo.\n",
    "\n",
    "Exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summation of multiple of 20 from 1 to 1000:  25500\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(range(1, 1001))\n",
    "\n",
    "# accumulator\n",
    "summation_20_mult = sc.accumulator(0)\n",
    "\n",
    "def conditional_print(i):\n",
    "    if i % 20 == 0: \n",
    "        print(i)\n",
    "        summation_20_mult.add(i)\n",
    "     \n",
    "\n",
    "data.foreach(conditional_print)\n",
    "\n",
    "print ('Summation of multiple of 20 from 1 to 1000: ', summation_20_mult.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Links:\n",
    "* Lista mais completa com mais funções comuns: http://spark.apache.org/docs/1.6.3/programming-guide.html#actions\n",
    "* Documentação da API RDD do Spark: http://spark.apache.org/docs/1.6.3/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.2\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
